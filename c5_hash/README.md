# 定义

散列技术能支持常数平均时间的插入、查找、删除，但不支持排序。

理想的散列表是一个带有关键字的具有固定大小的数组，大小我们定义为 `tsize`。使用一个散列函数将关键字的值映射到散列表的一个位置并进行存储。因为散列表的大小有限而关键字可能的取值范围要大得多，散列函数的冲突是不可避免的，因此还需要一个方法来处理冲突。（以及优化散列表的大小）

### 散列函数

散列函数最重要的性质应该是**分配均匀**。这跟输入也有很大关系，输入在其取值范围内分布不均是常见现象。主要体现在

- 可能集中在少数小区间
- 可能具有某种规律，比如同余

当输入是整数的时候，可以对表大小取余。取余算法尽可能均匀的方法是使用素数作为表大小，这时表的素数因子最少。

当输入是字符串的时候，冲突的概率更高。我们参考 md5 的实现，会看到除了均匀以外，散列函数还需要一个性质是能够处理任意字符长度。

### 开放散列（分离链接）

基本思路是使用一个链表存储冲突的元素。使用链表的原因是预期冲突的情况不多，也可以使用查找树或其他结构来代替。

我们定义装填因子（load factor）（λ）为哈希表中元素数量/哈希表大小。此时链表的平均长度等于装填因子。那么查找的过程就是一次散列+一次链表的遍历。大约为（1 + 1/2 *  λ），因为平均遍历到一半就能找到。

### 封闭散列（开放寻址）

分离链接法的缺点是需要分配新的内存，这会导致程序变慢。开放寻址法在处理冲突时的选择是继续进行运算，直到找到空位为止。开放寻址就是找到空位的意思。

可见这种算法对相同输入的计算结果可能是不同的，它像是有状态的。因为第一顺位的单元可能已经被占用了。因此除了关键字，还有一个输入是尝试顺序，设输入为 X，尝试次数为 i，目标单元为 h，那么有以下公式：

    h(X, i) = (Hash(X) + F(i)) mod TableSize
    且 F(0) = 0

其中 F 是冲突解决函数。就是定义下一个尝试单元索引的函数。

可见 i 最终的值会随装填因子的增大而增大，因此一般会控制装填因子小于 0.5。

另外也不能执行标准的删除操作，因为可能因为在 i=n 处删出的空位导致之前 i>n 处插入的元素失联。

最简单的**线性探测** F 可以是 `F(i) = i`，这等于顺序查找。优点是在表填满之前总能找到空位，缺点是容易形成聚集（primary clustering）。这会导致落入聚集区间的 Hash(X) 尝试太多次。通过分析聚集生成的概率，最终可以得到结论，在装填因子小于 0.5 时线性探测表现还不错，但是大于 0.5 时表现很差。

避免聚集的另一种二次探测函数是 `F(i) = x ^ 2`。但它有个问题是在表被填满之前就可能找不到空位。甚至于我们只能证明：

    如果使用平方探测，且表的大小是素数，那么当表至少有一半是空的时候，总能够插入一个新元素。

或者在某些特定条件下，比如：

    如果表的大小是形如 4k + 3 的素数，且 F(i) = ±(i ^ 2)，那么整个表都可以被探测到。

双散列是在 F(i) 中再使用一个新的散列函数，比如 `F(i) = i * NewHash(X)`。这会牺牲一些计算性能，但均匀效果一般比二次探测要好。

### 再散列（rehash）

当装填因子变得过大的时候，我们可以新创建一个更大的表，比如 2 倍以上的素数大小。然后把已存在的元素全部**重新插入** 到新表里，这个过程就叫做再散列。

这使得散列表可以支持动态大小的数据，但再散列发生时会有一段时间某种程度的使用受限（如果不是完全不可用的话）。现实中的实现细节可以参考 redis。

Python 中有一个对象：`collections.ChainMap`，功能与 rehashing 相近但做法完全不同。rehashing 的目的是给哈希表扩容，以期继续发挥正常的哈希表功能。ChainMap 则是把若干已存在的散列表连接起来，从而避免内存分配与再散列过程的开销。但提供封装过的，与散列表一致的接口。如果目标散列表已经不打算继续使用、单纯想把几个哈希表的内容合并读取，那么 ChainMap 是个好选择。

### 可扩展散列（Extendible hashing）

可扩展散列针对的是数据太多而不得不存放在磁盘上的情形，此时寻址操作的成本远大于其他。不论是开放还是封闭散列，冲突解决方法都有可能在一次查询中导致多次寻址，同时再散列的成本也会过高。这种场景下我们期望哈希过程能够在尽可能少的寻址过程中找到关键字，并且再散列过程也影响尽可能少的块。

在可扩展散列中，我们把固定大小的磁盘块称为桶（bucket），这是真正存储数据的哈希表，我们假设其可存储 m 条数据。还有一个内存中的，存储桶地址的数据结构，我们称为目录（directories）。当散列函数计算出一个值以后，我们通过目录就能查出这个值的目标桶。如果桶溢出，那么执行桶分裂操作，这个桶里的数据执行再散列，但其他桶不变。由此我们得到了一个可扩展的散列表。

目录使用的算法是这样的：先把散列值转换成整数，然后取 D 位进行查找，获取桶的地址。即目录内有一个大小为 `2**D` 的数组。当桶溢出时，D++，目录大小也翻倍，因为要保持低位结果的一致性，具体的翻倍方法其实就是 `dir *= 2`，然后溢出的目录再散列。

这里没有解释桶内的数据结构因为其选择相对独立。可以看到可扩展散列本质上是一个多级（二级）结构，优势在于再散列过程的成本较小。而如果单看目录的散列算法，我们会发现其实它的效率很糟糕，因为它处理冲突（此处为桶溢出）的方法是散列表翻倍。通过复杂的计算可以得出桶的数量期望是`(n/m)log2(e)`，其中 `log2(e)≈1.44`，目录大小的期望值是 O(n**(1/m+1)/m)。当 m=1 时，可扩展散列退化为单级结构，其目录大小期望为 O(n**2/2)，这显然不可接受。即这种算法的选择其实是牺牲目录的空间效率换取哈希表扩（缩）容时再散列的执行效率。

那么如果在目录中应用普通哈希表能获得好的再散列效率吗？或者说在保证再散列效率的基础上，目录还有没有更好的数据结构可选？我想不到，哈哈。

### 一致性散列（Consistent Hashing）

和可扩展散列有类似效果的另一种散列是一致性散列。它同样追求再散列时只影响一个桶，但在约束上存在区别——一致性散列不要求所有桶具有相同的大小，而是认为桶的大小是任意的。起关键作用的是一个环，或者叫圆周，每个桶连接到圆周上的一个点，并尽可能分散。散列函数将关键字映射到圆周上，然后开始顺时针或逆时针查找最近的桶。可见每新增或减少一个桶，都只会影响其相邻的一个桶（即再散列）。

一致性散列的提出最初是用于建设“分布式缓存”系统，因此认为桶（单台缓存结点）大小是任意的这点是成立的，关键的指标并不是桶容量，而是桶的访问次数，即 QPS。
